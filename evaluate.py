import argparse
import glob
import json
import os
import warnings
from pathlib import Path

import open_clip as clip
import numpy as np
import pandas as pd
import sklearn.preprocessing
import torch
from packaging import version
from PIL import Image
from torchvision.transforms import CenterCrop, Compose, Normalize, Resize, ToTensor
from tqdm import tqdm
import pdb


class CLIPCapDataset(torch.utils.data.Dataset):
    def __init__(self, data, append=False, prefix='A photo depicts'):
        self.data = data
        self.prefix = ''
        if append:
            self.prefix = prefix
            if self.prefix[-1] != ' ':
                self.prefix += ' '

    def __getitem__(self, idx):
        c_data = list(self.data[idx].values())[0]
        # c_data = clip.tokenize(self.prefix + c_data, truncate=True).squeeze()
        #thum_code
        c_data = clip.tokenize(self.prefix + c_data).squeeze()
        return {'caption': c_data}

    def __len__(self):
        return len(self.data)


def Convert(image):
    return image.convert("RGB")


class CLIPImageDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self.data = data
        # only 224x224 ViT-B/32 supported for now
        self.preprocess = self._transform_test(224)

    def _transform_test(self, n_px):
        return Compose([
            Resize(n_px, interpolation=Image.BICUBIC),
            CenterCrop(n_px),
            Convert,
            ToTensor(),
            Normalize((0.48145466, 0.4578275, 0.40821073),
                      (0.26862954, 0.26130258, 0.27577711)),
        ])

    def __getitem__(self, idx):
        c_data = self.data[idx]
        image = Image.open(c_data)
        image = self.preprocess(image)
        return {'image': image}

    def __len__(self):
        return len(self.data)


class DINOImageDataset(torch.utils.data.Dataset):
    def __init__(self, data):
        self.data = data
        # only 224x224 ViT-B/32 supported for now
        self.preprocess = self._transform_test(224)

    def _transform_test(self, n_px):
        return Compose([
            Resize(256, interpolation=Image.BICUBIC),
            CenterCrop(n_px),
            Convert,
            ToTensor(),
            Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
        ])

    def __getitem__(self, idx):
        c_data = self.data[idx]
        image = Image.open(c_data)
        image = self.preprocess(image)
        return {'image': image}

    def __len__(self):
        return len(self.data)


def extract_all_captions(captions, model, device, batch_size=256, num_workers=8, append=False):
    data = torch.utils.data.DataLoader(
        CLIPCapDataset(captions, append=append),
        batch_size=batch_size, num_workers=num_workers, shuffle=False)
    all_text_features = []
    with torch.no_grad():
        for b in tqdm(data):
            b = b['caption'].to(device)
            all_text_features.append(model.encode_text(b).cpu().numpy())
    all_text_features = np.vstack(all_text_features)
    return all_text_features


def extract_all_images(images, model, datasetclass, device, batch_size=64, num_workers=8):
    data = torch.utils.data.DataLoader(
        datasetclass(images),
        batch_size=batch_size, num_workers=num_workers, shuffle=False)
    all_image_features = []
    with torch.no_grad():
        for b in tqdm(data):
            b = b['image'].to(device)
            if hasattr(model, 'encode_image'):
                if device == 'cuda':
                    b = b.to(torch.float)
                all_image_features.append(model.encode_image(b).cpu().numpy())
            else:
                all_image_features.append(model(b).cpu().numpy())
    all_image_features = np.vstack(all_image_features)
    return all_image_features


def get_clip_score(model, images, candidates, device, append=False, w=2.5):
    '''
    get standard image-text clipscore.
    images can either be:
    - a list of strings specifying filepaths for images
    - a precomputed, ordered matrix of image features
    '''
    if isinstance(images, list):
        # need to extract image features
        images = extract_all_images(images, model, device)

    candidates = extract_all_captions(candidates, model, device, append=append)

    # as of numpy 1.21, normalize doesn't work properly for float16
    if version.parse(np.__version__) < version.parse('1.21'):
        images = sklearn.preprocessing.normalize(images, axis=1)
        candidates = sklearn.preprocessing.normalize(candidates, axis=1)
    else:
        warnings.warn(
            'due to a numerical instability, new numpy normalization is slightly different than paper results. '
            'to exactly replicate paper results, please use numpy version less than 1.21, e.g., 1.20.3.')
        images = images / np.sqrt(np.sum(images ** 2, axis=1, keepdims=True))
        candidates = candidates / \
            np.sqrt(np.sum(candidates ** 2, axis=1, keepdims=True))

    per = w * np.clip(np.sum(images * candidates, axis=1), 0, None)
    return np.mean(per), per, candidates


def clipeval(image_dir, candidates_json, device):
    image_paths = [os.path.join(image_dir, path) for path in os.listdir(image_dir)
                   if path.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.JPG'))]
    image_ids = [Path(path).stem for path in image_paths]
    with open(candidates_json) as f:
        candidates = json.load(f)
    pdb.set_trace()
    candidates = [candidates[int(cid)-1] for cid in image_ids]

    # model, _ = clip.load("ViT-B/32", device=device, jit=False)
    #thum_code
    model, _, preprocess = clip.create_model_and_transforms('ViT-B-32', device=device, pretrained='laion2b_s34b_b79k') 

    model.eval()

    image_feats = extract_all_images(
        image_paths, model, CLIPImageDataset, device, batch_size=64, num_workers=8)

    _, per_instance_image_text, _ = get_clip_score(
        model, image_feats, candidates, device)

    scores = {image_id: {'CLIPScore': float(clipscore)}
              for image_id, clipscore in
              zip(image_ids, per_instance_image_text)}
    print('CLIPScore: {:.4f}'.format(
        np.mean([s['CLIPScore'] for s in scores.values()])))

    return np.mean([s['CLIPScore'] for s in scores.values()]), np.std([s['CLIPScore'] for s in scores.values()])


def clipeval_image(image_dir, image_dir_ref, device):
    image_paths = [os.path.join(image_dir, path) for path in os.listdir(image_dir)
                   if path.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.JPG'))]
    image_paths_ref = [os.path.join(image_dir_ref, path) for path in os.listdir(image_dir_ref)
                       if path.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.JPG'))]

    # model, _ = clip.load("ViT-B/32", device=device, jit=False)
    # thum_code
    model, _, preprocess = clip.create_model_and_transforms('ViT-B-32', device=device, pretrained='laion2b_s34b_b79k') 

    model.eval()

    image_feats = extract_all_images(
        image_paths, model, CLIPImageDataset, device, batch_size=64, num_workers=8)

    image_feats_ref = extract_all_images(
        image_paths_ref, model, CLIPImageDataset, device, batch_size=64, num_workers=8)

    image_feats = image_feats / \
        np.sqrt(np.sum(image_feats ** 2, axis=1, keepdims=True))
    image_feats_ref = image_feats_ref / \
        np.sqrt(np.sum(image_feats_ref ** 2, axis=1, keepdims=True))
    res = image_feats @ image_feats_ref.T
    return np.mean(res)


def dinoeval_image(image_dir, image_dir_ref, device):
    image_paths = [os.path.join(image_dir, path) for path in os.listdir(image_dir)
                   if path.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.JPG'))]
    image_paths_ref = [os.path.join(image_dir_ref, path) for path in os.listdir(image_dir_ref)
                       if path.endswith(('.png', '.jpg', '.jpeg', '.tiff', '.JPG'))]

    model = torch.hub.load('facebookresearch/dino:main', 'dino_vits16').to(device)
    model.eval()

    image_feats = extract_all_images(
        image_paths, model, DINOImageDataset, device, batch_size=64, num_workers=8)

    image_feats_ref = extract_all_images(
        image_paths_ref, model, DINOImageDataset, device, batch_size=64, num_workers=8)

    image_feats = image_feats / \
        np.sqrt(np.sum(image_feats ** 2, axis=1, keepdims=True))
    image_feats_ref = image_feats_ref / \
        np.sqrt(np.sum(image_feats_ref ** 2, axis=1, keepdims=True))
    res = image_feats @ image_feats_ref.T
    return np.mean(res)


def calmetrics(sample_root, target_paths, numgen, outpkl, outcsv):
    device = 'cuda'

    outpath = os.path.dirname(outpkl)
    os.makedirs(outpath, exist_ok=True)

    if os.path.exists(outpkl):
        df = pd.read_pickle(outpkl)
    else:
        df = pd.DataFrame()
    full = {}

    assert sample_root.is_dir()
    image_path = sample_root / 'samples'
    json_path = sample_root / 'prompts.json'
    # pdb.set_trace()

    # assert len(glob.glob(str(image_path / '*.jpg'))) == numgen, "Sample folder does not contain required number of images"

    textalignment, _ = \
        clipeval(str(image_path), str(json_path), device)

    sd = {}
    sd['CLIP Text alignment'] = textalignment

    for i, target_path in enumerate(target_paths.split('+')):
        imagealignment = \
            clipeval_image(str(image_path), target_path, device)

        dinoimagealignment = \
            dinoeval_image(str(image_path), target_path, device)
        if i > 0:
            sd[f'CLIP Image alignment{i}'] = imagealignment
            sd[f'DINO Image alignment{i}'] = dinoimagealignment
        else:
            sd['CLIP Image alignment'] = imagealignment
            sd['DINO Image alignment'] = dinoimagealignment

    expname = sample_root
    if expname not in full:
        full[expname] = sd
    else:
        full[expname] = {**sd, **full[expname]}
    print(sd)

    print("Metrics:", full)

    for expname, sd in full.items():
        if expname not in df.index:
            df1 = pd.DataFrame(sd, index=[expname])
            df = pd.concat([df, df1])
        else:
            df.loc[df.index == expname, sd.keys()] = sd.values()

    df.to_pickle(outpkl)
    df.to_csv(outcsv)

    '''
    import pickle as pkl
    import pandas as pd
    with open("file.pkl", "rb") as f:
        object = pkl.load(f)
        
    df = pd.DataFrame(object)
    df.to_csv(r'file.csv')
    '''


def parse_args():
    parser = argparse.ArgumentParser("metric", add_help=False)
    parser.add_argument("--sample_root", type=str,
                        help="the root folder to generated images")
    parser.add_argument("--numgen", type=int, default=100,
                        help="total number of images.")
    parser.add_argument("--target_paths", type=str,
                        help="+ separated paths to real target images")
    parser.add_argument("--outpkl", type=str, default="evaluation.pkl",
                        help="the path to save result pkl file")
    parser.add_argument("--outcsv", type=str, default="evaluation.csv",
                        help="the path to save result csv file")
    
    return parser.parse_args()


def main(args):
    calmetrics(Path(args.sample_root), args.target_paths,
               args.numgen, args.outpkl, args.outcsv)


if __name__ == "__main__":
    # distributed setting
    args = parse_args()
    main(args)
